Directory structure:
└── CareerZone-AI-CHATBOT/
    ├── README.md
    ├── requirements.txt
    ├── test.http
    ├── api/
    │   ├── main.py
    │   ├── routes.py
    │   └── __pycache__/
    ├── config/
    │   ├── settings.py
    │   └── __pycache__/
    ├── core/
    │   ├── db.py
    │   └── llm.py
    ├── schemas/
    │   └── common.py
    ├── scripts/
    │   └── initial_load.py
    ├── services/
    │   └── rag_service.py
    └── workers/
        └── kafka_consumer.py

================================================
File: README.md
================================================
# CareerZone-AI-CHATBOT

```bash
uvicorn api.main:app --reload --port 8000



================================================
File: requirements.txt
================================================
fastapi
uvicorn[standard]
pydantic
python-dotenv
langchain
langchain-community
langchain-core
langchain-google-genai
langchain-mongodb
pymongo
unstructured
python-docx
kafka-python
tqdm



================================================
File: test.http
================================================
POST http://localhost:8000/api/v1/chat
Content-Type: application/json

{
  "query": "Chào bạn, tôi muốn tìm hiểu về các chính sách của công ty.",
  "history": []
}

###

POST http://localhost:8000/api/v1/chat
Content-Type: application/json

{
  "query": "tóm tắt chúng",
  "history": [
    {
      "role": "user",
      "content": "Chào bạn, tôi muốn tìm hiểu về các chính sách của công ty."
    },
    {
      "role": "assistant",
      "content": "Chào bạn, tôi là CareerConnect AI. Tôi có thể giúp gì cho bạn về các chính sách của công ty?"
    }
  ]
}



================================================
File: api/main.py
================================================
from fastapi import FastAPI
from api.routes import router as chat_router

app = FastAPI(title="Smart RAG Chatbot Service")
app.include_router(chat_router, prefix="/api/v1")



================================================
File: api/routes.py
================================================
from fastapi import APIRouter
from schemas.common import ChatRequest
from services.rag_service import process_query_stream
from fastapi.responses import StreamingResponse

router = APIRouter()

@router.post("/chat")
async def chat_endpoint(request: ChatRequest):
    return StreamingResponse(process_query_stream(request.query, request.history), media_type="text/event-stream")




================================================
File: config/settings.py
================================================
import os
from dotenv import load_dotenv
load_dotenv()

MONGODB_URI = os.getenv("MONGODB_URI")
DB_NAME = os.getenv("DB_NAME")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS")
KAFKA_JOB_EVENTS_TOPIC = os.getenv("KAFKA_JOB_EVENTS_TOPIC")




================================================
File: core/db.py
================================================
from pymongo import MongoClient
from config import settings

mongo_client = MongoClient(settings.MONGODB_URI)
db = mongo_client[settings.DB_NAME]



================================================
File: core/llm.py
================================================
# core/llm.py
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

from config.settings import GOOGLE_API_KEY

# Khởi tạo một lần và tái sử dụng
embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=GOOGLE_API_KEY
)

# Mô hình để sinh câu trả lời (không streaming)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite-preview-06-17",
    google_api_key=GOOGLE_API_KEY,
    temperature=0.1
)

# Mô hình để trích xuất metadata (không streaming)
structured_llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite-preview-06-17",
    temperature=0,
    google_api_key=GOOGLE_API_KEY
)



================================================
File: schemas/common.py
================================================
from pydantic import BaseModel
from typing import List, Optional

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    query: str
    history: List[ChatMessage] = []



================================================
File: scripts/initial_load.py
================================================
# FILE: scripts/initial_load.py
import os
import sys
from typing import List, Optional

from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_mongodb import MongoDBAtlasVectorSearch
from pydantic import BaseModel, Field

from core.db import db
from core.llm import embedding_model, structured_llm

# Add project root to Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


# --- Pydantic Models for Data Enrichment ---
class JobMetadata(BaseModel):
    """
    Mô hình Pydantic để chứa metadata được LLM trích xuất cho một công việc.
    """
    category: str = Field(
        description="Phân loại ngành nghề của công việc, ví dụ: 'Software Development', 'Marketing', 'Data Science'.")
    level: str = Field(
        description="Cấp bậc của công việc, ví dụ: 'Intern', 'Junior', 'Senior', 'Manager'.")
    skills: List[str] = Field(
        description="Danh sách các kỹ năng công nghệ, phần mềm, hoặc chuyên môn cụ thể được yêu cầu.")
    keywords: List[str] = Field(
        description="Các từ khóa chung hoặc thuật ngữ nghiệp vụ khác liên quan đến công việc.")


class Job(BaseModel):
    """Mô hình Pydantic đại diện cho một công việc thô."""
    title: str
    page_content: str  # Description of the job


def load_policies():
    """Tải, chia nhỏ và nạp dữ liệu chính sách."""
    print("--- Bắt đầu xử lý file chính sách ---")
    loader = TextLoader("data/policies.txt", encoding="utf-8")
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(docs)
    print(f"Đã chia file chính sách thành {len(chunks)} chunks.")
    policies_collection = db["policies_vector"]
    policies_collection.delete_many({})
    MongoDBAtlasVectorSearch.from_documents(
        documents=chunks,
        embedding=embedding_model,
        collection=policies_collection,
        index_name="policies_vector_index"
    )
    print("--- Hoàn thành nạp dữ liệu chính sách ---")


def load_jobs():
    """
    Nạp dữ liệu jobs.
    Tự động trích xuất và làm giàu metadata cho jobs bằng LLM.
    """
    jobs_collection = db["jobs_vector"]
    jobs_collection.delete_many({})

    raw_job_data = [
        {"title": "Senior Python Developer", "page_content": "Senior Python Developer. Yêu cầu 5 năm kinh nghiệm với Django, Flask. Ưu tiên có kinh nghiệm với DevOps. Xây dựng các hệ thống backend và API.", "jobId": "job_001"},
        {"title": "Junior Frontend Developer",
            "page_content": "Junior Frontend Developer (ReactJS). Yêu cầu kiến thức về HTML, CSS, JavaScript và React. Xây dựng giao diện người dùng cho ứng dụng web.", "jobId": "job_002"},
        {"title": "Marketing Manager", "page_content": "Marketing Manager. Lập kế hoạch và triển khai các chiến dịch marketing online. Có kinh nghiệm về SEO, SEM, và Google Analytics. Quản lý đội ngũ marketing.", "jobId": "job_003"},
        {"title": "Data Scientist", "page_content": "Data Scientist. Cần có kinh nghiệm làm việc với các mô hình Machine Learning, Deep Learning. Sử dụng thành thạo Python và các thư viện như Scikit-learn, TensorFlow.", "jobId": "job_004"},
        {"title": "Nhân viên Kế toán Tổng hợp",
            "page_content": "Cần tuyển Kế toán tổng hợp có kinh nghiệm 2 năm. Thành thạo phần mềm MISA, Excel. Chịu trách nhiệm báo cáo thuế, báo cáo tài chính. Cẩn thận, trung thực.", "jobId": "job_005"}
    ]

    enriched_job_docs = []
    for job_data in raw_job_data:
        # Kết hợp metadata đã trích xuất với dữ liệu gốc
        full_metadata = {
            "source": "jobs",
            "title": job_data["title"],
            "jobId": job_data["jobId"]
        }

        # Tạo Document hoàn chỉnh
        doc = Document(
            page_content=job_data["page_content"], metadata=full_metadata)
        enriched_job_docs.append(doc)

    # 4. Nạp các documents đã được làm giàu vào Vector Store
    MongoDBAtlasVectorSearch.from_documents(
        documents=enriched_job_docs,
        embedding=embedding_model,
        collection=jobs_collection,
        index_name="default"
    )
    print(f"Đã nạp {len(enriched_job_docs)} jobs đã được làm giàu metadata.")

    print("--- Hoàn thành nạp dữ liệu ---")


if __name__ == "__main__":
    load_policies()
    load_jobs()



================================================
File: services/rag_service.py
================================================
# FILE: services/rag_service.py

import json
from operator import itemgetter
from typing import Any, AsyncGenerator, Dict, List, Optional

from langchain.chains.router.llm_router import RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.prompts import PromptTemplate
from langchain.schema import AIMessage, HumanMessage
from langchain.schema.runnable import Runnable
from langchain_core.callbacks.manager import (
    AsyncCallbackManagerForRetrieverRun,
    CallbackManagerForRetrieverRun,
)
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnablePassthrough
from langchain_mongodb import MongoDBAtlasVectorSearch
from pydantic import BaseModel, Field

from core.db import db
from core.llm import embedding_model, llm
from schemas.common import ChatMessage


# 1. ĐỊNH NGHĨA PYDANTIC MODEL CHO FILTERS (Không đổi)
class JobFilters(BaseModel):
    """Cấu trúc chỉ chứa bộ lọc keywords"""
    keywords: Optional[List[str]] = Field(
        None, description="Các từ khóa chung hoặc thuật ngữ nghiệp vụ chữ thường, ví dụ: ['backend', 'api', 'python']."
    )


AVAILABLE_LEVELS = ["Intern", "Junior", "Senior", "Lead", "Manager"]


# CẬP NHẬT: Gắn thẻ nguồn vào metadata của document để biết nó đến từ đâu
class MultiSourceRetriever(BaseRetriever):
    """
    Retriever sử dụng router để chọn retriever phù hợp.
    CẬP NHẬT: Gắn thẻ nguồn (tên retriever) vào metadata của mỗi document.
    """
    retrievers: Dict[str, BaseRetriever]
    router: Runnable

    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:
        result = self.router.invoke({"input": query}, config={
                                    "callbacks": run_manager.get_child()})
        destination = result.get('destination')

        if destination and destination in self.retrievers:
            print(f"Router chose (sync): {destination}")
            chosen_retriever = self.retrievers[destination]
            docs = chosen_retriever.get_relevant_documents(
                query, callbacks=run_manager.get_child())
            for doc in docs:
                doc.metadata["source_retriever"] = destination
            return docs

        print("Router could not choose a destination (sync). Querying all retrievers.")
        all_docs = []
        for name, retriever in self.retrievers.items():
            docs = retriever.get_relevant_documents(
                query, callbacks=run_manager.get_child())
            for doc in docs:
                doc.metadata["source_retriever"] = name
            all_docs.extend(docs)
        return all_docs

    async def _aget_relevant_documents(self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun) -> List[Document]:
        result = await self.router.ainvoke({"input": query}, config={"callbacks": run_manager.get_child()})
        destination = result.get('destination')

        if destination and destination in self.retrievers:
            print(f"Router chose (async): {destination}")
            chosen_retriever = self.retrievers[destination]
            docs = await chosen_retriever.ainvoke(query, config={"callbacks": run_manager.get_child()})
            for doc in docs:
                doc.metadata["source_retriever"] = destination
            return docs

        print("Router could not choose a destination (async). Querying all retrievers.")
        all_docs = []
        for name, retriever in self.retrievers.items():
            docs = await retriever.ainvoke(query, config={"callbacks": run_manager.get_child()})
            for doc in docs:
                doc.metadata["source_retriever"] = name
            all_docs.extend(docs)
        return all_docs


# 2. XÂY DỰNG GET_RETRIEVER (Không đổi)
def get_retriever(job_filters: Optional[JobFilters] = None):
    vs_jobs = MongoDBAtlasVectorSearch(
        collection=db["jobs_vector"],
        embedding=embedding_model,
        index_name="default"
    )
    search_kwargs = {'k': 10}
    if job_filters and job_filters.keywords:
        normalized_keywords = [k.lower() for k in job_filters.keywords]
        keyword_filters = [
            {"keywords": {"$in": [keyword]}}
            for keyword in normalized_keywords
        ]
        search_kwargs['pre_filter'] = {"$or": keyword_filters}
        print(f"Applying keyword filters: {search_kwargs['pre_filter']}")
    jobs_retriever = vs_jobs.as_retriever(search_kwargs=search_kwargs)

    vs_policies = MongoDBAtlasVectorSearch(
        collection=db["policies_vector"], embedding=embedding_model, index_name="default")
    policies_retriever = vs_policies.as_retriever(search_kwargs={'k': 3})

    retriever_infos = [
        {"name": "recruitment", "description": "Hữu ích cho các câu hỏi về tuyển dụng, tìm kiếm công việc, ứng viên và hồ sơ.",
         "retriever": jobs_retriever},
        {"name": "company_policies", "description": "Hữu ích cho các câu hỏi về quy định, điều khoản dịch vụ, chính sách bảo mật của công ty.",
         "retriever": policies_retriever}
    ]
    router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations="\n".join(
        [f'{r["name"]}: {r["description"]}' for r in retriever_infos]))
    router_prompt = PromptTemplate.from_template(router_template)
    lcel_router = router_prompt | llm | RouterOutputParser()

    return MultiSourceRetriever(retrievers={info["name"]: info["retriever"] for info in retriever_infos}, router=lcel_router)


def format_docs(docs: List[Document]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)


def _format_chat_history(chat_history: List[ChatMessage]):
    buffer = []
    for msg in chat_history:
        if msg.role == "user":
            buffer.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            buffer.append(AIMessage(content=msg.content))
    return buffer


# CẬP NHẬT: Luồng xử lý chính được cấu trúc lại để trả về jobId
async def process_query_stream(query: str, history: List[ChatMessage]) -> AsyncGenerator[str, None]:
    """
    Xử lý câu hỏi, bao gồm trích xuất filter, RAG, và trả về cả câu trả lời lẫn danh sách job ID.
    """
    # # 1. Trích xuất filter từ câu hỏi (không đổi)
    # filter_extraction_prompt = PromptTemplate.from_template(
    #     """Dựa vào câu hỏi của người dùng, hãy trích xuất các tiêu chí lọc cho việc làm.
    #     Nếu không có thông tin, hãy để trống. Câu hỏi người dùng: {query}"""
    # )
    # filter_extraction_chain = (
    #     filter_extraction_prompt | llm.with_structured_output(JobFilters)
    # )
    # extracted_filters = await filter_extraction_chain.ainvoke({"query": query})
    # print(f"Extracted job filters: {extracted_filters}")

    # retriever = get_retriever(job_filters=extracted_filters)
    retriever = get_retriever()

    # 2. Xác định câu hỏi độc lập nếu có lịch sử trò chuyện
    _template = """Với lịch sử trò chuyện sau đây và một câu hỏi theo sau, hãy diễn đạt lại câu hỏi đó thành một câu hỏi độc lập.
    Lịch sử trò chuyện: {chat_history}
    Câu hỏi theo sau: {question}
    Câu hỏi độc lập:"""
    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

    input_query = query
    if history:
        print("History found, creating standalone question.")
        standalone_question_chain = (
            {"question": itemgetter(
                "question"), "chat_history": lambda x: _format_chat_history(x["chat_history"])}
            | CONDENSE_QUESTION_PROMPT
            | llm
            | StrOutputParser()
        )
        input_query = await standalone_question_chain.ainvoke({"question": query, "chat_history": history})
        print(f"Standalone question: {input_query}")

    # 3. Lấy tài liệu (jobs, policies, etc.) MỘT LẦN DUY NHẤT
    retrieved_docs = await retriever.ainvoke(input_query)
    # print(f"Retrieved {(retrieved_docs)} documents.")
    context_str = format_docs(retrieved_docs)

    # 4. Tạo và stream câu trả lời từ LLM
    qa_template = """Bạn là "CareerZone AI", một trợ lý tuyển dụng ảo thông minh.
    Sử dụng ngữ cảnh sau để trả lời câu hỏi. Nếu không biết, hãy nói bạn không biết.
    NGỮ CẢNH: --- {context} ---
    CÂU HỎI: {question}
    TRẢ LỜI:"""
    QA_PROMPT = PromptTemplate.from_template(qa_template)
    qa_chain = QA_PROMPT | llm | StrOutputParser()

    async for chunk in qa_chain.astream({"context": context_str, "question": input_query}):
        response_packet = {"type": "answer_chunk", "data": chunk}
        yield f"data: {json.dumps(response_packet, ensure_ascii=False)}\n\n"

    # # 5. Lọc và gửi danh sách các jobId đã tìm được
    # job_ids = []
    # job_docs = [doc for doc in retrieved_docs if doc.metadata.get("source_retriever") == "recruitment"]

    # for doc in job_docs:
    #     # Lấy jobId từ metadata
    #     job_id = doc.metadata.get("jobId")
    #     if job_id:
    #         # Chuyển đổi sang str để đảm bảo tương thích JSON (phòng trường hợp nó là ObjectId)
    #         job_ids.append(str(job_id))

    # if job_ids:
    #     print(f"Found {len(job_ids)} job IDs to return.")
    #     response_packet = {"type": "source_job_ids", "data": job_ids}
    #     yield f"data: {json.dumps(response_packet, ensure_ascii=False)}\n\n"

    # # Gửi tín hiệu kết thúc stream
    # end_packet = {"type": "stream_end"}
    # yield f"data: {json.dumps(end_packet, ensure_ascii=False)}\n\n"



================================================
File: workers/kafka_consumer.py
================================================
import json
from kafka import KafkaConsumer
from config import settings
from core.db import db
from core.llm import embedding_model
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime

class JobPayload(BaseModel):
    id: str
    title: str
    description: str
    requirements: str
    benefits: Optional[str] = ""
    location: dict
    minSalary: Optional[int] = 0
    maxSalary: Optional[int] = 0
    deadline: datetime

class JobEvent(BaseModel):
    action: str  # "CREATE", "UPDATE", "DELETE"
    payload: JobPayload

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

def _prepare_documents(job: JobPayload) -> list[Document]:
    """Chuẩn bị văn bản và metadata từ một job để embedding."""
    content = (
        f"Tiêu đề công việc: {job.title}\n"
        f"Mô tả: {job.description}\n"
        f"Yêu cầu: {job.requirements}\n"
        f"Phúc lợi: {job.benefits}\n"
        f"Địa điểm: {job.location.get('city')}, {job.location.get('district')}"
    )
    metadata = {
        "source": "job_posting",
        "job_id": job.id,
        "title": job.title,
        "city": job.location.get('city'),
        "deadline": job.deadline.isoformat()
    }
    doc = Document(page_content=content, metadata=metadata)
    split_docs = text_splitter.split_documents([doc])
    return split_docs

def upsert_job(job: JobPayload):
    """Thêm mới hoặc cập nhật một job vào vector store."""
    job_id = job.id
    print(f"Bắt đầu UPSERT cho job_id: {job_id}")
    jobs_collection = db["jobs_vector"]
    vs_jobs = MongoDBAtlasVectorSearch(
        collection=jobs_collection,
        embedding=embedding_model,
        index_name="jobs_vector_index"
    )
    vs_jobs.delete(ids=[job_id])
    documents = _prepare_documents(job)
    vs_jobs.add_documents(documents, ids=[job_id for _ in documents])
    print(f"  - Đã thêm {len(documents)} chunks mới cho job_id: {job_id}")

def delete_job(job_id: str):
    """Xóa tất cả các chunks liên quan đến một job_id."""
    print(f"Bắt đầu DELETE cho job_id: {job_id}")
    jobs_collection = db["jobs_vector"]
    vs_jobs = MongoDBAtlasVectorSearch(
        collection=jobs_collection,
        embedding=embedding_model,
        index_name="jobs_vector_index"
    )
    vs_jobs.delete(ids=[job_id])
    print(f"  - Đã xóa thành công các chunks của job_id: {job_id}")

def start_consumer():
    consumer = KafkaConsumer(
        settings.KAFKA_JOB_EVENTS_TOPIC,
        bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest'
    )
    print("Kafka consumer started. Waiting for job events...")

    for message in consumer:
        try:
            event_data = message.value
            job_event = JobEvent(**event_data)

            action = job_event.action.upper()
            payload = job_event.payload

            print(f"Received event: {action} for job_id: {payload.id}")

            if action in ["CREATE", "UPDATE"]:
                upsert_job(payload)
            elif action == "DELETE":
                delete_job(payload.id)
            else:
                print(f"Unknown action: {action}")

        except Exception as e:
            print(f"Error processing message: {message.value}. Error: {e}")

if __name__ == "__main__":
    start_consumer()


